{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('recipes': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5e8cf52ad1b7483d318bf58df08a289b53708a54660830216f8f9d43f61ffd81"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-core azureml-widgets azureml-train-core azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# AzureML libraries\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Datastore, Run, Environment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.get(name='', subscription_id='', resource_group='')\n",
    "\n",
    "# Print workspace attributes\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Workspace region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datastore from blob storage containing training data.\n",
    "# Consult README.md for instructions downloading and uploading training data.\n",
    "ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name='',\n",
    "                                             account_name='', \n",
    "                                             account_key='',\n",
    "                                             container_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print datastore attributes\n",
    "print('Datastore name: ' + ds.name, \n",
    "      'Container name: ' + ds.container_name, \n",
    "      'Datastore type: ' + ds.datastore_type, \n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the compute cluster\n",
    "gpu_cluster_name = \"ndv2-cluster\" \n",
    "\n",
    "# Verify that the cluster doesn't exist already\n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_ND40rs_v2', min_nodes=0, max_nodes=4)\n",
    "    \n",
    "    # create the cluster\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 1\n",
    "experiment_name = 'hf-t5-ortmodule'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_t5_large_fp16_env = Environment(name=\"hf-t5-large-fp16-env\")\n",
    "hf_t5_large_fp16_env.docker.enabled = True\n",
    "hf_t5_large_fp16_env.python.user_managed_dependencies = True\n",
    "#hf_t5_large_fp16_env.environment_variables = {\"CUDA_VISIBLE_DEVICES\":\"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the container registry information.\n",
    "hf_t5_large_fp16_env.docker.base_image = ''\n",
    "hf_t5_large_fp16_env.docker.base_image_registry.address = ''\n",
    "hf_t5_large_fp16_env.docker.base_image_registry.username = ''\n",
    "hf_t5_large_fp16_env.docker.base_image_registry.password = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_ref = ds.path('tst-translation').as_mount() #str(output_dir_ref)\n",
    "args = ['--source_prefix', 'translate English to Romanian:', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--model_name_or_path', 't5-large', '--output_dir', '/tmp/tst-translation', '--adam_eps', '1e-06', '--do_train', '--label_smoothing', 0.1, '--learning_rate', '3e-5', '--logging_first_step', '--logging_steps', 1000, '--max_source_length', 128, '--max_target_length', 128, '--num_train_epochs', 1, '--overwrite_output_dir', '--per_device_train_batch_size', 32, '--predict_with_generate', '--sortish_sampler', '--task', 'translation_en_to_ro', '--warmup_steps', 5, '--max_train_samples', 1024, '--fp16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "hf_t5_large_fp16_src = ScriptRunConfig(source_directory='.',\n",
    "                      script='run_seq2seq.py',\n",
    "                      arguments=args,\n",
    "                      compute_target=gpu_compute_target,\n",
    "                      environment=hf_t5_large_fp16_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = experiment.submit(hf_t5_large_fp16_src,  tags={'model':'large', 'config':'fp16', 'gpus':'1'})\n",
    "#run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_args = \"--source_prefix \\'translate English to Romanian:\\' --dataset_name wmt16 --dataset_config \\'ro-en\\' --model_name_or_path t5-large --output_dir /tmp/tst-translation --adam_eps 1e-06 --do_train --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_train_batch_size 32 --predict_with_generate --sortish_sampler --task translation_en_to_ro --warmup_steps 5 --max_train_samples 1024 --fp16\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create distributed config\n",
    "distr_config = PyTorchConfiguration(process_count=2, node_count=1)\n",
    "hf_t5_large_fp16_env.environment_variables = {\"CUDA_VISIBLE_DEVICES\":\"0,1\"}\n",
    "\n",
    "# create job config\n",
    "hf_t5_large_fp16_multigpu_src = ScriptRunConfig(source_directory='.',\n",
    "                                                script='run_seq2seq.py',\n",
    "                                                arguments=dist_args,\n",
    "                                                compute_target=gpu_compute_target,\n",
    "                                                environment=hf_t5_large_fp16_env,\n",
    "                                                distributed_job_config=distr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "\n",
    "# create distributed config\n",
    "distr_config = PyTorchConfiguration(process_count=2, node_count=1)\n",
    "\n",
    "# define command\n",
    "multigpu_cmd = \"python -m torch.distributed.launch --nproc_per_node 2 --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT --use_env run_seq2seq.py --source_prefix 'translate English to Romanian:' --dataset_name wmt16 --dataset_config 'ro-en' --model_name_or_path t5-large --output_dir /tmp/tst-translation --adam_eps 1e-06 --do_train --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_train_batch_size 32 --predict_with_generate --sortish_sampler --task translation_en_to_ro --warmup_steps 5 --max_train_samples 1024 --fp16\"\n",
    "\n",
    "# create job config\n",
    "hf_t5_large_fp16_multigpu_src = ScriptRunConfig(source_directory='.',\n",
    "                                command=multigpu_cmd,\n",
    "                                compute_target=gpu_compute_target,\n",
    "                                environment=hf_t5_large_fp16_env,\n",
    "                                distributed_job_config=distr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multigpu_run = experiment.submit(hf_t5_large_fp16_multigpu_src,  tags={'model':'large', 'config':'fp16', 'gpus':'2'})\n",
    "#multigpu_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ort_args = ['--source_prefix', 'translate English to Romanian:', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--model_name_or_path', 't5-large', '--output_dir', '/tmp/tst-translation', '--adam_eps', '1e-06', '--do_train', '--label_smoothing', 0.1, '--learning_rate', '3e-5', '--logging_first_step', '--logging_steps', 1000, '--max_source_length', 128, '--max_target_length', 128, '--num_train_epochs', 1, '--overwrite_output_dir', '--per_device_train_batch_size', 32, '--predict_with_generate', '--sortish_sampler', '--task', 'translation_en_to_ro', '--warmup_steps', 5, '--max_train_samples', 1024, '--fp16', '--ort', '--deepspeed', 'ds_config_zero_0.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "\n",
    "# define command\n",
    "ds_ort_cmd = [\"python run_seq2seq.py \", str(ds_ort_args)]\n",
    "\n",
    "# create job config\n",
    "hf_t5_large_fp16_ds_ort_src = ScriptRunConfig(source_directory='.',\n",
    "                                              command=ds_ort_cmd,\n",
    "                                              compute_target=gpu_compute_target,\n",
    "                                              environment=hf_t5_large_fp16_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_ort_run = experiment.submit(hf_t5_large_fp16_ds_ort_src,  tags={'model':'large', 'config':'ds_ort', 'gpus':'1'})\n",
    "#ds_ort_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create distributed config\n",
    "multinode_distr_config = PyTorchConfiguration(node_count=2)\n",
    "\n",
    "# define command\n",
    "launch_cmd = [\"CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node 1 --nnodes 2\" \\\n",
    "              \"run_seq2seq.py \", args]\n",
    "\n",
    "# create job config\n",
    "hf_t5_large_fp16_multinode_src = ScriptRunConfig(source_directory='.',\n",
    "                                command=launch_cmd,\n",
    "                                compute_target=gpu_compute_target,\n",
    "                                environment=hf_t5_large_fp16_env,\n",
    "                                distributed_job_config=multinode_distr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinode_run = experiment.submit(hf_t5_large_fp16_multinode_src,  tags={'model':'large', 'config':'fp16', 'nodes':'2'})\n",
    "#multinode_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}